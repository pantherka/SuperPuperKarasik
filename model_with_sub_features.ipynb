{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import os\n",
    "import gzip\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import chain        \n",
    "from contextlib import ExitStack \n",
    "import time\n",
    "import json\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "from multiprocessing.dummy import Lock as ThreadLock \n",
    "from multiprocessing.dummy import Value as ThreadValue\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import cosine\n",
    "from multiprocessing.dummy import Lock\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.fasttext import load_facebook_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titles_stemming.json', 'r', encoding='utf8') as file:\n",
    "    vec = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for value in vec.values():\n",
    "    sentences.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fasttext = FastText('ft_freqprune_400K_100K_pq_300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28026"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем сделать общий файл с эмеддингами текстов.. не просто так же их парсили"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем эмбединги и запоминаем их"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись в файл стринговых эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем прочитать эмбеддинги - не забудь перевести в float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts_embeddings.json', 'r', encoding='utf8') as file:\n",
    "    texts_embedding_from_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in texts_embedding_from_json.items():\n",
    "    embedding = np.array(list(map(float, texts_embedding_from_json[key])))\n",
    "    texts_embedding_from_json[key] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts_embedding_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если фасттекст применять к текстам после стемминга, результат получается лучше почему-то."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для заголовков:\n",
    "1) Метки кластеров внутри группы\n",
    "2) Силуэт для объекта группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Import:\n",
    "    def __init__(self):\n",
    "        self.doc_to_title = {}\n",
    "        #self.model_fasttext = FastText('/Users/egor/Desktop/Техносфера/MachineLearning/project/ft_freqprune_400K_100K_pq_300.bin')\n",
    "        with open('docs_titles.tsv', encoding='utf-8') as f:\n",
    "            for num_line, line in enumerate(f):\n",
    "                if num_line == 0:\n",
    "                    continue\n",
    "                data = line.strip().split('\\t', 1)\n",
    "                doc_id = int(data[0])\n",
    "                if len(data) == 1:\n",
    "                    title = ''\n",
    "                else:\n",
    "                    title = data[1]\n",
    "                self.doc_to_title[doc_id] = title\n",
    "        self.doc_to_vec = TfidfVectorizer().fit_transform([self.doc_to_title[i] for i in self.doc_to_title.keys()])\n",
    "        self.top_k_cosine = 5 # k ближайших косинусных расстояний\n",
    "        self.n_features_jac = 3\n",
    "        self.extra_features = self.cosine\n",
    "   \n",
    "    def cosine(self, group):\n",
    "        \n",
    "        def jaccard(group):\n",
    "            X = np.empty(shape=(group.size, self.n_features_jac), dtype=np.float)\n",
    "            for i, doc_ID in enumerate(group):\n",
    "                dist = np.empty(shape=(group.size, ), dtype=np.float)\n",
    "                vec = set(self.doc_to_title[doc_ID])\n",
    "                for j, doc_j in enumerate(group):\n",
    "                    if doc_ID == doc_j:\n",
    "                        dist[j] = 0\n",
    "                    else:\n",
    "                        vec_j = set(self.doc_to_title[doc_j])\n",
    "                        intersec = len(vec.intersection(vec_j))\n",
    "                        if intersec > 0:\n",
    "                            dist[j] = intersec/(len(vec) + len(vec_j) - intersec)\n",
    "                        else:\n",
    "                            dist[j] = 0\n",
    "                X[i] = np.sort(dist)[-self.n_features_jac:]\n",
    "            return X\n",
    "    \n",
    "        n = (self.n_features)//2\n",
    "        X = np.empty(shape=(group.size, self.n_features), dtype=np.float)\n",
    "        for i, title in enumerate(pairwise_distances(self.doc_to_vec[group], metric='cosine')):\n",
    "            X[i, :n] = sorted(title)[1:n + 1]\n",
    "        X[:, n:] = np.mean(X[:, :n], axis=0)\n",
    "        X[:, :n] /= X[:, n:]\n",
    "        if (self.n_features_jac==0):\n",
    "            return X\n",
    "        X_jac = jaccard(group)\n",
    "        return np.hstack((X,X_jac))\n",
    "                \n",
    "    \n",
    "    def make_train(self):\n",
    "        train_data = pd.read_csv('train_groups.csv', encoding='utf-8')\n",
    "        traingroups_titledata = {}\n",
    "        for i in range(len(train_data)):\n",
    "            new_doc = train_data.iloc[i]\n",
    "            doc_group = new_doc['group_id']\n",
    "            doc_id = new_doc['doc_id']\n",
    "            target = new_doc['target']\n",
    "            title = self.doc_to_title[doc_id]\n",
    "            if doc_group not in traingroups_titledata:\n",
    "                traingroups_titledata[doc_group] = []\n",
    "            traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "        \n",
    "        y_train = []\n",
    "        X_train = []\n",
    "        groups_train = []\n",
    "        for new_group in traingroups_titledata:\n",
    "            docs = traingroups_titledata[new_group]\n",
    "            for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "                y_train.append(target_id)\n",
    "                groups_train.append(new_group)\n",
    "                all_dist = []\n",
    "                words = set(title.strip().split())\n",
    "                for j in range(0, len(docs)):\n",
    "                    if k == j:\n",
    "                        continue\n",
    "                    doc_id_j, title_j, target_j = docs[j]\n",
    "                    words_j = set(title_j.strip().split())\n",
    "                    all_dist.append(len(words.intersection(words_j)))\n",
    "                X_train.append(sorted(all_dist, reverse=True)[0:25]    )\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        groups_train = np.array(groups_train)\n",
    "        \n",
    "        # Класстеризация внутри группы:\n",
    "        i = 0\n",
    "        My_X = np.zeros((train_data.shape[0], self.top_k_cosine + 2)) # кол-во новых фичей\n",
    "        groups = train_data.groupby('group_id')\n",
    "        rand_index = []\n",
    "        for group_id, group_indx in groups.groups.items():\n",
    "            group = train_data.iloc[group_indx]\n",
    "            docs_id = group.doc_id.values\n",
    "            # сначала заберем все файлы из группы\n",
    "            embedded_texts = []\n",
    "            group_titles = []\n",
    "            for idx in docs_id:\n",
    "                group_titles.append(vec[str(idx)])\n",
    "                embedded_texts.append(texts_embedding_from_json[str(idx)])\n",
    "            embedded_titles = model_fasttext.wv[group_titles]\n",
    "            targets = group.target.values\n",
    "            model_titles = DBSCAN(0.475, metric=\"cosine\", min_samples=7)\n",
    "            labels_titles = model_titles.fit_predict(embedded_titles)\n",
    "            if np.unique(labels_titles).size > 1:\n",
    "                silhouette = silhouette_samples(embedded_titles, labels_titles)\n",
    "            else:\n",
    "                silhouette = np.ones(len(labels_titles)) # если всего один кластер\n",
    "            model_texts = DBSCAN(0.7, metric=\"cosine\", min_samples=5)\n",
    "            labels_texts = model_texts.fit_predict(embedded_texts)\n",
    "            # добавим фичи из расстояний\n",
    "            # получим матрицу из попарных расстояний\n",
    "            cos_distances = cosine_similarity(embedded_titles)\n",
    "            sorted_dist = np.apply_along_axis(np.sort, axis=1, arr=cos_distances)[:, ::-1]\n",
    "            cos_features = sorted_dist[:, 1: self.top_k_cosine + 1]\n",
    "            My_X[i: i + group_indx.size, :self.top_k_cosine] = cos_features\n",
    "            My_X[i: i + group_indx.size, -2] = labels_titles\n",
    "            My_X[i: i + group_indx.size, -1] = labels_texts\n",
    "            #My_X[i: i + group_indx.size, -2] = silhouette\n",
    "            #My_X[i: i + group_indx.size, :-2] = self.extra_features(group.doc_id)\n",
    "            i += group_indx.size\n",
    "        \n",
    "        \n",
    "        X_train = np.concatenate((X_train, My_X), axis=1)\n",
    "        return X_train, y_train, groups_train\n",
    "    \n",
    "    def make_test(self):\n",
    "        test_data = pd.read_csv('test_groups.csv', encoding='utf-8')\n",
    "        testgroups_titledata = {}\n",
    "        for i in range(len(test_data)):\n",
    "            new_doc = test_data.iloc[i]\n",
    "            doc_group = new_doc['group_id']\n",
    "            doc_id = new_doc['doc_id']\n",
    "            title = self.doc_to_title[doc_id]\n",
    "            if doc_group not in testgroups_titledata:\n",
    "                testgroups_titledata[doc_group] = []\n",
    "            testgroups_titledata[doc_group].append((doc_id, title))\n",
    "            \n",
    "        X_test = []\n",
    "        groups_test = []\n",
    "        for new_group in testgroups_titledata:\n",
    "            docs = testgroups_titledata[new_group]\n",
    "            for k, (doc_id, title) in enumerate(docs):\n",
    "                groups_test.append(new_group)\n",
    "                all_dist = []\n",
    "                words = set(title.strip().split())\n",
    "                for j in range(0, len(docs)):\n",
    "                    if k == j:\n",
    "                        continue\n",
    "                    doc_id_j, title_j = docs[j]\n",
    "                    words_j = set(title_j.strip().split())\n",
    "                    all_dist.append(len(words.intersection(words_j)))\n",
    "                X_test.append(sorted(all_dist, reverse=True)[0:25])\n",
    "        X_test = np.array(X_test)\n",
    "        \n",
    "        i = 0\n",
    "        My_X = np.zeros((test_data.shape[0], self.top_k_cosine + 2)) # кол-во новых фичей\n",
    "        groups = test_data.groupby('group_id')\n",
    "        for group_id, group_indx in groups.groups.items():\n",
    "            group = test_data.iloc[group_indx]\n",
    "            docs_id = group.doc_id.values\n",
    "            group_titles = []\n",
    "            embedded_texts = []\n",
    "            for idx in docs_id:\n",
    "                group_titles.append(vec[str(idx)])\n",
    "                embedded_texts.append(texts_embedding_from_json[str(idx)])\n",
    "            embedded_titles = model_fasttext.wv[group_titles]\n",
    "            model_titles = DBSCAN(0.475, metric=\"cosine\", min_samples=7)\n",
    "            labels_titles = model_titles.fit_predict(embedded_titles)\n",
    "            if np.unique(labels_titles).size > 1:\n",
    "                silhouette = silhouette_samples(embedded_titles, labels_titles)\n",
    "            else:\n",
    "                silhouette = np.ones(len(labels_titles)) # если всего один кластер\n",
    "            model_texts = DBSCAN(0.7, metric=\"cosine\", min_samples=5)\n",
    "            labels_texts = model_texts.fit_predict(embedded_texts)\n",
    "            cos_distances = cosine_similarity(embedded_titles)\n",
    "            sorted_dist = np.apply_along_axis(np.sort, axis=1, arr=cos_distances)[:, ::-1]\n",
    "            cos_features = sorted_dist[:, 1: self.top_k_cosine + 1]\n",
    "            My_X[i: i + group_indx.size, :self.top_k_cosine] = cos_features\n",
    "            My_X[i: i + group_indx.size, -2] = labels_titles\n",
    "            My_X[i: i + group_indx.size, -1] = labels_texts\n",
    "            My_X[i: i + group_indx.size, -2] = labels_titles\n",
    "            My_X[i: i + group_indx.size, -1] = labels_texts\n",
    "            #My_X[i: i + group_indx.size, -2] = silhouette\n",
    "            #My_X[i: i + group_indx.size, :-2] = self.extra_features(group.doc_id)\n",
    "            i += group_indx.size\n",
    "        \n",
    "        X_test = np.concatenate((X_test, My_X), axis=1)\n",
    "        groups_test = np.array(groups_test)\n",
    "        return X_test, groups_test, test_data['pair_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, groups_train = importer.make_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, groups_test, pair_ids = importer.make_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модельки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator\n",
    "from statistics import mean\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import catboost\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voiting(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.clf_e_1 = RandomForestClassifier(max_depth=10)\n",
    "        self.clf_e_2 = ExtraTreesClassifier(max_depth=15)\n",
    "        self.clf_e_3 = LogisticRegression()\n",
    "        self.clf_e_4 = catboost.CatBoostClassifier()\n",
    "        self.clf_e_5 = MLPClassifier(max_iter=800, hidden_layer_sizes=(10, 10), random_state=42)\n",
    "        self.clf_e_6 = KNeighborsClassifier(10)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf_e_1.fit(X, y)\n",
    "        self.clf_e_2.fit(X, y)\n",
    "        self.clf_e_3.fit(X, y)\n",
    "        self.clf_e_4.fit(X, y)\n",
    "        self.clf_e_5.fit(X, y)\n",
    "        self.clf_e_6.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.pred = np.transpose(np.array([self.clf_e_1.predict(X),\n",
    "                                self.clf_e_2.predict(X),\n",
    "                                self.clf_e_3.predict(X),\n",
    "                                self.clf_e_4.predict(X),\n",
    "                                self.clf_e_5.predict(X),\n",
    "                                self.clf_e_6.predict(X)]))\n",
    "        def pr(line):\n",
    "            ar, count = np.unique(line, return_counts = True)\n",
    "            return ar[np.argmax(count)]\n",
    "        predict = np.array([pr(line) for line in self.pred])\n",
    "        return predict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator(X_train, X_test, num_model):\n",
    "    indices = np.array([])\n",
    "    length = X_train.shape[1]\n",
    "    features = np.arange(length)\n",
    "    np.random.shuffle(features)\n",
    "    for i in range(num_model):\n",
    "        ind = np.random.randint(0, length, length)\n",
    "        ind = np.unique(np.append(ind, features[i * (length // num_model) : (i + 1) * (length // num_model + 1)]))\n",
    "        yield X_train[:, ind], X_test[:, ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sk = StandardScaler()\n",
    "X_train_sk = my_sk.fit_transform(X_train)\n",
    "X_test_sk = my_sk.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator(X_train, X_test, num_model):\n",
    "    indices = np.array([])\n",
    "    length = X_train.shape[1]\n",
    "    features = np.arange(length)\n",
    "    np.random.shuffle(features)\n",
    "    for i in range(num_model):\n",
    "        ind = np.random.randint(0, length, length)\n",
    "        ind = np.unique(np.append(ind, features[i * (length // num_model) : (i + 1) * (length // num_model + 1)]))\n",
    "        yield X_train[:, ind], X_test[:, ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_generator = feature_generator(X_train_sk, X_test_sk, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ada, X_test_ada = next(X_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 112 candidates, totalling 1120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "C:\\Users\\Maria\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done 1120 out of 1120 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier()\n",
    "\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(X_train_ada,y_train)\n",
    "\n",
    "ada_best = gsadaDTC.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='random'),\n",
       "          learning_rate=1.5, n_estimators=2, random_state=7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ada = ada_best.predict_proba(X_train_ada)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4466148428675835\n",
      "0.4466148428675835\n",
      "0.9977734896838355\n",
      "0.9976239976239976\n",
      "0.9971764006538861\n",
      "0.9974721189591078\n",
      "0.9974645786726323\n"
     ]
    }
   ],
   "source": [
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_ada > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extra, X_test_extra = next(X_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "C:\\Users\\Maria\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=4)]: Done 540 out of 540 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6898228464348698"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExtC = ExtraTreesClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsExtC.fit(X_train_extra,y_train)\n",
    "\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_extra = ExtC_best.predict_proba(X_train_extra)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.656619803782745\n",
      "0.7287468701561942\n",
      "0.7588859416445624\n",
      "0.7670512633270045\n",
      "0.7543075941289088\n",
      "0.7173231090972465\n",
      "0.6448382126348229\n"
     ]
    }
   ],
   "source": [
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_extra > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res, X_test_res = next(X_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "C:\\Users\\Maria\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=4)]: Done 540 out of 540 | elapsed: 14.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6901521996520792"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RFC Parameters tunning \n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsRFC.fit(X_train_sk,y_train)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6756048387096774\n",
      "0.7485673352435531\n",
      "0.7757912859223944\n",
      "0.7855064027939465\n",
      "0.7760623905777494\n",
      "0.7396830924603867\n",
      "0.6681965959074393\n"
     ]
    }
   ],
   "source": [
    "res_forest = RFC_best.predict_proba(X_train_sk)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_forest > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gbc, X_test_gbc = next(X_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "C:\\Users\\Maria\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done 720 out of 720 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6950218722882998"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient boosting tunning\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [100,200,300],\n",
    "              'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.1] \n",
    "              }\n",
    "\n",
    "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsGBC.fit(X_train_gbc,y_train)\n",
    "\n",
    "GBC_best = gsGBC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 32 and input n_features is 27 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-52d8ba8e171c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGBC_best\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_gbc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mth\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_grad\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   2082\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m         \"\"\"\n\u001b[1;32m-> 2084\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score_to_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1990\u001b[0m         \"\"\"\n\u001b[0;32m   1991\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1992\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1993\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# for use in inner loop, not raveling the output in single-class case,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \u001b[1;31m# not doing input validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1586\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m         \u001b[0mpredict_stages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_init_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1574\u001b[0m         \u001b[1;34m\"\"\"Check input and compute prediction of ``init``. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1575\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1576\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    386\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 32 and input n_features is 27 "
     ]
    }
   ],
   "source": [
    "res_grad = GBC_best.predict_proba(X_train_gbc)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_grad > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svc, X_test_svc = next(X_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 11.3min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0c7eb36a6a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgsSVMC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVMC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgsSVMC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mSVMC_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgsSVMC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/sphere-py38/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### SVC classifier\n",
    "SVMC = SVC(probability=True)\n",
    "svc_param_grid = {'kernel': ['rbf'], \n",
    "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "                  'C': [1, 10, 50, 100,200,300, 1000]}\n",
    "\n",
    "gsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsSVMC.fit(X_train_res,y_train) # да, тут res, потому что в RandomForest было обучение на всех фичах\n",
    "\n",
    "SVMC_best = gsSVMC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsSVMC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_svc = SVMC_best.predict_proba(X_train_res)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_svc > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pls = (res_ada + res_extra + res_forest + res_grad)/ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6936640186010463"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train, np.array(res_pls > th, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n",
    "\n",
    "votingC = votingC.fit(X_train_sk, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pls = np.array(votingC.predict_proba(X_test_sk)[:, 1] > 0.4, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subpobeda.csv\", \"w\", encoding='utf-8') as outfile:\n",
    "    outfile.write(str(\"pair_id,target\\n\"))\n",
    "    for i in range(len(y_pls)):\n",
    "        outfile.write(str(pair_ids[i]) + str(\",\") + str(y_pls[i]) + str(\"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
