{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import os\n",
    "import gzip\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import chain        \n",
    "from contextlib import ExitStack \n",
    "import time\n",
    "import json\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "from multiprocessing.dummy import Lock as ThreadLock \n",
    "from multiprocessing.dummy import Value as ThreadValue\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import cosine\n",
    "from multiprocessing.dummy import Lock\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator\n",
    "from statistics import mean\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import catboost\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = {}\n",
    "with open('titles_stemming.json', 'r', encoding='utf8') as file:\n",
    "    vec = json.load(file)\n",
    "sentences = []\n",
    "for value in vec.values():\n",
    "    sentences.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fasttext = FastText('ft_freqprune_400K_100K_pq_300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts_embeddings.json', 'r', encoding='utf8') as file:\n",
    "    texts_embedding_from_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in texts_embedding_from_json.items():\n",
    "    embedding = np.array(list(map(float, texts_embedding_from_json[key])))\n",
    "    texts_embedding_from_json[key] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Import:\n",
    "    def __init__(self):\n",
    "        self.doc_to_title = {}\n",
    "        #self.model_fasttext = FastText('/Users/egor/Desktop/Техносфера/MachineLearning/project/ft_freqprune_400K_100K_pq_300.bin')\n",
    "        with open('docs_titles.tsv') as f:\n",
    "            for num_line, line in enumerate(f):\n",
    "                if num_line == 0:\n",
    "                    continue\n",
    "                data = line.strip().split('\\t', 1)\n",
    "                doc_id = int(data[0])\n",
    "                if len(data) == 1:\n",
    "                    title = ''\n",
    "                else:\n",
    "                    title = data[1]\n",
    "                self.doc_to_title[doc_id] = title\n",
    "        self.doc_to_vec = TfidfVectorizer().fit_transform([self.doc_to_title[i] for i in self.doc_to_title.keys()])\n",
    "        self.top_k_cosine_titles = 25 # k ближайших косинусных расстояний для заголовков\n",
    "        self.top_k_cosine_texts = 3 # k ближайших косинусных расстояний для заголовков\n",
    "    \n",
    "    def make_train(self):\n",
    "        train_data = pd.read_csv('train_groups.csv')\n",
    "        traingroups_titledata = {}\n",
    "        for i in range(len(train_data)):\n",
    "            new_doc = train_data.iloc[i]\n",
    "            doc_group = new_doc['group_id']\n",
    "            doc_id = new_doc['doc_id']\n",
    "            target = new_doc['target']\n",
    "            title = self.doc_to_title[doc_id]\n",
    "            if doc_group not in traingroups_titledata:\n",
    "                traingroups_titledata[doc_group] = []\n",
    "            traingroups_titledata[doc_group].append((doc_id, title, target))\n",
    "        \n",
    "        y_train = []\n",
    "        X_train = []\n",
    "        groups_train = []\n",
    "        for new_group in traingroups_titledata:\n",
    "            docs = traingroups_titledata[new_group]\n",
    "            for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "                y_train.append(target_id)\n",
    "                groups_train.append(new_group)\n",
    "                all_dist = []\n",
    "                words = set(title.strip().split())\n",
    "                for j in range(0, len(docs)):\n",
    "                    if k == j:\n",
    "                        continue\n",
    "                    doc_id_j, title_j, target_j = docs[j]\n",
    "                    words_j = set(title_j.strip().split())\n",
    "                    all_dist.append(len(words.intersection(words_j)))\n",
    "                X_train.append(sorted(all_dist, reverse=True)[0:25]    )\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        groups_train = np.array(groups_train)\n",
    "        \n",
    "        # Класстеризация внутри группы:\n",
    "        i = 0\n",
    "        My_X = np.zeros((train_data.shape[0], self.top_k_cosine_titles + 2)) # кол-во новых фичей\n",
    "        groups = train_data.groupby('group_id')\n",
    "        rand_index = []\n",
    "        for group_id, group_indx in groups.groups.items():\n",
    "            group = train_data.iloc[group_indx]\n",
    "            docs_id = group.doc_id.values\n",
    "            # сначала заберем все файлы из группы\n",
    "            embedded_texts = []\n",
    "            group_titles = []\n",
    "            for idx in docs_id:\n",
    "                group_titles.append(vec[str(idx)])\n",
    "                embedded_texts.append(texts_embedding_from_json[str(idx)])\n",
    "            embedded_titles = model_fasttext.wv[group_titles]\n",
    "            targets = group.target.values\n",
    "            model_titles = DBSCAN(0.6, metric=\"cosine\", min_samples=8)\n",
    "            labels_titles = model_titles.fit_predict(embedded_titles)\n",
    "            if np.unique(labels_titles).size > 1:\n",
    "                silhouette = silhouette_samples(embedded_titles, labels_titles)\n",
    "            else:\n",
    "                silhouette = np.ones(len(labels_titles)) # если всего один кластер\n",
    "            model_texts = DBSCAN(0.1, metric=\"cosine\", min_samples=5)\n",
    "            labels_texts = model_texts.fit_predict(embedded_texts)\n",
    "            # добавим фичи из расстояний\n",
    "            # получим матрицу из попарных расстояний\n",
    "            cos_distances = cosine_similarity(embedded_titles)\n",
    "            sorted_dist = np.apply_along_axis(np.sort, axis=1, arr=cos_distances)[:, ::-1]\n",
    "            cos_features = sorted_dist[:, 1: self.top_k_cosine_titles + 1]\n",
    "            \n",
    "            cos_distances_texts = cosine_similarity(embedded_texts)\n",
    "            sorted_dist_texts = np.apply_along_axis(np.sort, axis=1, arr=cos_distances_texts)[:, ::-1]\n",
    "            cos_features_texts = sorted_dist_texts[:, 1: self.top_k_cosine_texts + 1]\n",
    "            \n",
    "            \n",
    "            My_X[i: i + group_indx.size, :self.top_k_cosine_titles] = cos_features\n",
    "            My_X[i: i + group_indx.size, -2] = labels_titles\n",
    "            My_X[i: i + group_indx.size, -1] = labels_texts\n",
    "            i += group_indx.size\n",
    "        \n",
    "        \n",
    "        X_train = np.concatenate((X_train, My_X), axis=1)\n",
    "        return X_train, y_train, groups_train\n",
    "    \n",
    "    def make_test(self):\n",
    "        test_data = pd.read_csv('test_groups.csv')\n",
    "        testgroups_titledata = {}\n",
    "        for i in range(len(test_data)):\n",
    "            new_doc = test_data.iloc[i]\n",
    "            doc_group = new_doc['group_id']\n",
    "            doc_id = new_doc['doc_id']\n",
    "            title = self.doc_to_title[doc_id]\n",
    "            if doc_group not in testgroups_titledata:\n",
    "                testgroups_titledata[doc_group] = []\n",
    "            testgroups_titledata[doc_group].append((doc_id, title))\n",
    "            \n",
    "        X_test = []\n",
    "        groups_test = []\n",
    "        for new_group in testgroups_titledata:\n",
    "            docs = testgroups_titledata[new_group]\n",
    "            for k, (doc_id, title) in enumerate(docs):\n",
    "                groups_test.append(new_group)\n",
    "                all_dist = []\n",
    "                words = set(title.strip().split())\n",
    "                for j in range(0, len(docs)):\n",
    "                    if k == j:\n",
    "                        continue\n",
    "                    doc_id_j, title_j = docs[j]\n",
    "                    words_j = set(title_j.strip().split())\n",
    "                    all_dist.append(len(words.intersection(words_j)))\n",
    "                X_test.append(sorted(all_dist, reverse=True)[0:25])\n",
    "        X_test = np.array(X_test)\n",
    "        \n",
    "        i = 0\n",
    "        My_X = np.zeros((test_data.shape[0], self.top_k_cosine_titles + 2)) # кол-во новых фичей\n",
    "        groups = test_data.groupby('group_id')\n",
    "        for group_id, group_indx in groups.groups.items():\n",
    "            group = test_data.iloc[group_indx]\n",
    "            docs_id = group.doc_id.values\n",
    "            group_titles = []\n",
    "            embedded_texts = []\n",
    "            for idx in docs_id:\n",
    "                group_titles.append(vec[str(idx)])\n",
    "                embedded_texts.append(texts_embedding_from_json[str(idx)])\n",
    "            embedded_titles = model_fasttext.wv[group_titles]\n",
    "            model_titles = DBSCAN(0.6, metric=\"cosine\", min_samples=8)\n",
    "            labels_titles = model_titles.fit_predict(embedded_titles)\n",
    "            if np.unique(labels_titles).size > 1:\n",
    "                silhouette = silhouette_samples(embedded_titles, labels_titles)\n",
    "            else:\n",
    "                silhouette = np.ones(len(labels_titles)) # если всего один кластер\n",
    "            model_texts = DBSCAN(0.1, metric=\"cosine\", min_samples=5)\n",
    "            labels_texts = model_texts.fit_predict(embedded_texts)\n",
    "            cos_distances = cosine_similarity(embedded_titles)\n",
    "            sorted_dist = np.apply_along_axis(np.sort, axis=1, arr=cos_distances)[:, ::-1]\n",
    "            cos_features = sorted_dist[:, 1: self.top_k_cosine_titles + 1]\n",
    "            \n",
    "            cos_distances_texts = cosine_similarity(embedded_texts)\n",
    "            sorted_dist_texts = np.apply_along_axis(np.sort, axis=1, arr=cos_distances_texts)[:, ::-1]\n",
    "            cos_features_texts = sorted_dist_texts[:, 1: self.top_k_cosine_texts + 1]\n",
    "            \n",
    "            \n",
    "            My_X[i: i + group_indx.size, :self.top_k_cosine_titles] = cos_features\n",
    "            My_X[i: i + group_indx.size, -2] = labels_titles\n",
    "            My_X[i: i + group_indx.size, -1] = labels_texts\n",
    "            i += group_indx.size\n",
    "            \n",
    "            #My_X[i: i + group_indx.size, self.top_k_cosine_titles:self.top_k_cosine_titles\n",
    "            #     + self.top_k_cosine_texts] = cos_features_texts\n",
    "            # валидация дала DBSCAN(0.6, metric=\"cosine\", min_samples=8)\n",
    "        \n",
    "        X_test = np.concatenate((X_test, My_X), axis=1)\n",
    "        groups_test = np.array(groups_test)\n",
    "        return X_test, groups_test, test_data['pair_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, groups_train = importer.make_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, groups_test, pair_ids = importer.make_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voiting(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.clf_e_1 = RandomForestClassifier(max_depth=10)\n",
    "        self.clf_e_2 = ExtraTreesClassifier(max_depth=15)\n",
    "        self.clf_e_3 = LogisticRegression()\n",
    "        self.clf_e_4 = catboost.CatBoostClassifier()\n",
    "        self.clf_e_5 = MLPClassifier(max_iter=800, hidden_layer_sizes=(10, 10), random_state=42)\n",
    "        self.clf_e_6 = KNeighborsClassifier(10)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf_e_1.fit(X, y)\n",
    "        self.clf_e_2.fit(X, y)\n",
    "        self.clf_e_3.fit(X, y)\n",
    "        self.clf_e_4.fit(X, y)\n",
    "        self.clf_e_5.fit(X, y)\n",
    "        self.clf_e_6.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.pred = np.transpose(np.array([self.clf_e_1.predict(X),\n",
    "                                self.clf_e_2.predict(X),\n",
    "                                self.clf_e_3.predict(X),\n",
    "                                self.clf_e_4.predict(X),\n",
    "                                self.clf_e_5.predict(X),\n",
    "                                self.clf_e_6.predict(X)]))\n",
    "        def pr(line):\n",
    "            ar, count = np.unique(line, return_counts = True)\n",
    "            return ar[np.argmax(count)]\n",
    "        predict = np.array([pr(line) for line in self.pred])\n",
    "        return predict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sk = StandardScaler()\n",
    "X_train_sk = my_sk.fit_transform(X_train)\n",
    "X_test_sk = my_sk.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 112 candidates, totalling 560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done 560 out of 560 | elapsed:  2.3min finished\n"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier()\n",
    "\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(X_train_sk,y_train)\n",
    "\n",
    "ada_best = gsadaDTC.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4466148428675835\n",
      "0.4466148428675835\n",
      "0.9991082045184304\n",
      "0.9991082045184304\n",
      "0.9991082045184304\n",
      "0.9991066110780226\n",
      "0.9991066110780226\n"
     ]
    }
   ],
   "source": [
    "res_ada = ada_best.predict_proba(X_train_sk)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_ada > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done 270 out of 270 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6886489405661631"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExtC = ExtraTreesClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsExtC.fit(X_train_sk,y_train)\n",
    "\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6777193691872221\n",
      "0.7444523979957051\n",
      "0.7746031746031746\n",
      "0.7861827721906428\n",
      "0.7651951375559822\n",
      "0.7242769500438212\n",
      "0.6437984496124032\n"
     ]
    }
   ],
   "source": [
    "res_extra = ExtC_best.predict_proba(X_train_sk)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_extra > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   54.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=4)]: Done 270 out of 270 | elapsed: 16.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6930063619939852"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RFC Parameters tunning \n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsRFC.fit(X_train_sk,y_train)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7154108131119625\n",
      "0.8211611636185099\n",
      "0.8616675690308608\n",
      "0.8792143066549399\n",
      "0.8671065032987747\n",
      "0.8349547131834955\n",
      "0.7633977397010573\n"
     ]
    }
   ],
   "source": [
    "res_forest = RFC_best.predict_proba(X_train_sk)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_forest > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=4)]: Done 360 out of 360 | elapsed: 19.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6996043329287585"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient boosting tunning\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [100,200,300],\n",
    "              'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.1] \n",
    "              }\n",
    "\n",
    "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsGBC.fit(X_train_sk,y_train)\n",
    "\n",
    "GBC_best = gsGBC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6474201474201473\n",
      "0.7179239589619794\n",
      "0.7430332261521972\n",
      "0.7449498619386717\n",
      "0.7347970304849154\n",
      "0.7008635578583765\n",
      "0.6264921062764729\n"
     ]
    }
   ],
   "source": [
    "res_grad = GBC_best.predict_proba(X_train_sk)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_grad > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7130615786232939"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient boosting tunning\n",
    "\n",
    "XGB = xgb.XGBClassifier()\n",
    "xgb_param_grid = {'alpha' : [0.01, 0.1],\n",
    "              'n_estimators' : [10,20,30],\n",
    "              'eval_metric': ['auc'],\n",
    "              'max_depth': [4, 6],\n",
    "              'scale_pos_weight': [1, 2, 2.4] \n",
    "              }\n",
    "\n",
    "gsXGB = GridSearchCV(XGB,param_grid = xgb_param_grid, cv=kfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsXGB.fit(X_train_sk,y_train)\n",
    "\n",
    "XGB_best = gsXGB.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsXGB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5776690546915954\n",
      "0.6690579175018462\n",
      "0.7080266386260077\n",
      "0.7354214413037942\n",
      "0.7475192943770672\n",
      "0.7447723565178704\n",
      "0.7171717171717172\n"
     ]
    }
   ],
   "source": [
    "res_xgb = XGB_best.predict_proba(X_train_sk)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(res_xgb > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('rf', RFC_best), ('et', ExtC_best), ('xgb', XGB_best), ('gb', GBC_best)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот здесь я крутил модели и потом делал стакинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('rf',\n",
       "                                RandomForestClassifier(bootstrap=False,\n",
       "                                                       max_features=10,\n",
       "                                                       min_samples_leaf=10,\n",
       "                                                       min_samples_split=3)),\n",
       "                               ('et',\n",
       "                                ExtraTreesClassifier(max_features=10,\n",
       "                                                     min_samples_leaf=10,\n",
       "                                                     min_samples_split=3)),\n",
       "                               ('xgb',\n",
       "                                XGBClassifier(alpha=0.1, base_score=0.5,\n",
       "                                              booster='gbtree',\n",
       "                                              colsample_bylevel=1,\n",
       "                                              colsample_bynode=1,\n",
       "                                              colsample_bytree=1,\n",
       "                                              eval_metri...\n",
       "                                              monotone_constraints='()',\n",
       "                                              n_estimators=20, n_jobs=0,\n",
       "                                              num_parallel_tree=1,\n",
       "                                              random_state=0,\n",
       "                                              reg_alpha=0.100000001,\n",
       "                                              reg_lambda=1, scale_pos_weight=2,\n",
       "                                              subsample=1, tree_method='exact',\n",
       "                                              validate_parameters=1,\n",
       "                                              verbosity=None)),\n",
       "                               ('gb',\n",
       "                                GradientBoostingClassifier(learning_rate=0.05,\n",
       "                                                           max_depth=4,\n",
       "                                                           max_features=0.3,\n",
       "                                                           min_samples_leaf=100))],\n",
       "                   final_estimator=LogisticRegression())"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.fit(X_train_sk,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6769297017978254\n",
      "0.7373377449732756\n",
      "0.7503109023075861\n",
      "0.7529481132075471\n",
      "0.7352987545325557\n",
      "0.7127605490594814\n",
      "0.6491522265697782\n"
     ]
    }
   ],
   "source": [
    "stack_res = stack.predict_proba(X_train_sk)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(stack_res > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь ансамбль над стекингом (последний скор без этого был)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best), ('stack', stack), ('xgb', XGB_best)], voting='soft', n_jobs=4)\n",
    "\n",
    "votingC = votingC.fit(X_train_sk, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661152141802068\n",
      "0.7429577464788732\n",
      "0.7737491877842755\n",
      "0.7898161244695899\n",
      "0.7842418689876316\n",
      "0.765865063460254\n",
      "0.6934414148857775\n"
     ]
    }
   ],
   "source": [
    "vote = votingC.predict_proba(X_train_sk)[:, 1]\n",
    "for th in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print(f1_score(y_train, np.array(vote > th, dtype=int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ансамбль над стекингом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pls = np.array(votingC.predict_proba(X_test_sk)[:, 1] > 0.4, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "просто стекинг (у меня еще пороги разные получались иногда)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pls = np.array(stack.predict_proba(X_test_sk)[:, 1] > 0.4, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"subpobeda.csv\", \"w\", encoding='utf-8') as outfile:\n",
    "    outfile.write(str(\"pair_id,target\\n\"))\n",
    "    for i in range(len(y_pls)):\n",
    "        outfile.write(str(pair_ids[i]) + str(\",\") + str(y_pls[i]) + str(\"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
